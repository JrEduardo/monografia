% ------------------------------------------------------------------------
% CAPÍTULO 2 - REVISÃO DE LITERATURA
% ------------------------------------------------------------------------

Métodos para inferência em dados de contagem estão bem aquém da
quantidade disponível para dados contínuos. Destacamos o modelo
log-linear Poisson como o modelo mais utilizado quando se trata de dados
de contagem. Porém não raramente os dados de contagens apresentam
variância superior ou inferior à sua média. Esses são os casos de super
ou subdispersão já enunciados no capítulo \ref{cap:introducao}, que
quando ocorrem inviabilizam o uso da distribuição Poisson.

Nos casos de fuga da equidispersão algumas abordagens não paramétricas
são empregadas. Nesse contexto, podemos citar os métodos de estimação
via quase-verossimilhança, estimação robusta dos erros padrões
(estimador ``sanduíche'') e estimação dos erros padrões via reamostragem
(``\textit{bootstrap}'') \cite{Hilbe2014}. Desses métodos detalhamos
brevemente somente o método de estimação via função de
quase-verossimilhança na seção
\ref{cap02:estimacao-via-quase-verossimilhanca}.

No contexto paramétrico, pesquisas recentes trazem modelos bastante
flexíveis à fuga de equidispersão no campo da Estatística aplicada, veja
\cite{Sellers2010, Zeviani2014, Lord2010}. Na tabela
\ref{tab:distribuicoes} listamos as distribuições de
probabilidades consideradas por \citeonline{Winkelmann2008} e
\citeonline{Kokonendji2014} e as características de dados de contagem
que são contempladas. Notamos que a Poisson na verdade é um caso
particular, pois é a única das distribuições listada que contempla
somente a característica de equidipersão, ainda observa-se que temos um
conjunto maior de distribuições para os casos de superdispersão com
relação os casos de subdispersão. Embora este grande número de
distribuições exista para lidar com os casos de fuga de equidispersão
destacamos que são poucos os pacotes estatísticos que empregam essas
distribuições a modelos de regressão para dados de contagem.

%%----------------------------------------------------------------------
%% Tabela das distribuições para dados de contagem
\begin{table}
\centering
\caption{Distribuições de probabilidades para dados de contagem com
  indicação das características contempladas}
\label{tab:distribuicoes}
\begin{tabular}{lccc}
  \toprule
\multirow{2}{*}{Distribuição}       & \multicolumn{3}{c}{Contempla a característica de} \\
  \cline{2-4} \\[-0.3cm]
                                    & Equidispersão     & Superdispersão & Subdispersão \\[0.1cm]
  \hline
Poisson                             & \checkmark        &                &              \\
Binomial Negativa                   & \checkmark        & \checkmark     &              \\
\textit{Inverse Gaussian Poisson}   & \checkmark        & \checkmark     &              \\
\textit{Compound Poisson}           & \checkmark        & \checkmark     &              \\
Poisson Generalizada                & \checkmark        & \checkmark     & \checkmark   \\
\textit{Gamma-Count}                & \checkmark        & \checkmark     & \checkmark   \\
COM-Poisson                         & \checkmark        & \checkmark     & \checkmark   \\
Katz                                & \checkmark        & \checkmark     & \checkmark   \\
\textit{Poisson Polynomial}         & \checkmark        & \checkmark     & \checkmark   \\
\textit{Double-Poisson}             & \checkmark        & \checkmark     & \checkmark   \\
\textit{Lagrangian Poisson}         & \checkmark        & \checkmark     & \checkmark   \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
  \small
\item Fonte: Elaborado pelo autor.
\end{tablenotes}
\end{table}
%%----------------------------------------------------------------------

Dos modelos paramétricos o Binomial Negativo aparece em destaque com
implementações já consolidadas nos principais \textit{softwares}
estatísticos e frequentes aplicações nos casos de superdispersão. Na
seção \ref{cap02:binomneg} detalhes da construção desses modelos são
apresentados. Dos demais modelos derivados das distribuições listadas na
tabela \ref{tab:ditribuicoes} este trabalho abordará somente o
modelo COM-Poisson, que é apresentado com detalhes na seção
\ref{cap02:compoisson}.

Um outro fenômeno que é frequente em dados de contagem é a ocorrência
excessiva de zeros. Esse fenômeno sugere a modelagem de dois processos
geradores de dados, o gerador de zeros extra e o gerador das
contagens. Existem ao menos duas abordagens pertinentes para estes casos
que são os modelos de mistura e os modelos condicionais. Na abordagem
por modelos de mistura a variável resposta é modelada como uma mistura
de duas distribuições, no trabalho de \citeonline{Lambert1992},
uma mistura da distribuição Bernoulli com uma distribuição de Poisson ou
Binomial Negativa. Considerando os modelos condicionais, também chamados
de modelos de barreira \cite{Ridout1998}, temos que a modelagem da
variável resposta é realizada em duas etapas. A primeira refere-se ao
processo gerador de contagens nulas e a segunda ao gerador de contagens
não nulas. Nesta trabalho a modelagem de excesso de zeros se dará
somente via modelos de barreira. A seção \ref{cap02:zeros} é destinada a
um breve detalhamento desta abordagem.

Nesta capítulo também abordamos a situação da inclusão de efeitos
aleatórios no seção \ref{cap02:aleatorio}. Em análise de dados de
contagem a inclusão desses efeitos perimitem acomodar variabilidade
extra e incorporar a estrutura amostral do problema como em experimentos
com medidas repetidas ou longitudinais e experimentos em parcelas
subdivididas.

\section{Modelo Poisson}
\label{cap02:poisson}

A Poisson é uma das principais distribuição de probabilidades
discretas. Com suporte nos inteiros não negativos, dizemos que uma
variável aleatória segue um modelo Poisson se sua função massa de
probabilidade for

\begin{equation}
  \label{eqn:pmf-poisson}
  Pr(Y = y \mid \lambda) = \frac{\lambda^ye^{-\lambda}}{y!}
    \qquad y = 0, 1, 2, \cdots
\end{equation}

\noindent
em que $\lambda > 0$ representa a taxa de ocorrência do evento de
interesse. Uma particularidade já destacada desta distribuição é que
$E(X) = V(X) = \lambda$. Isso torna a distribuição Poisson bastante
reestritiva. Na figura \ref{fig:distr-poisson} são apresentadas as
ditribuições Poisson para diferentes parâmetros, note que devido a
propriedade $E(X) = V(X)$ contagens maiores também são mais dispersas.

<<distr-poisson, fig.cap="Probabilidades pela distribuição Poisson para diferentes valores de $\\lambda$", fig.height=3.5, fig.width=7>>=

lambdas <- c("p1" = 3, "p2" = 8, "p3" = 15)
y <- 0:30
py <- sapply(lambdas, function(p) dpois(y, p))
da <- as.data.frame(py)
da <- cbind(y, stack(da))

fl <- substitute(expression(
    lambda == p1, lambda == p2, lambda == p3),
    list(p1 = lambdas[1], p2 = lambdas[2], p3 = lambdas[3]))

xyplot(values ~ y | factor(ind), data = da,
       layout = c(NA, 1),
       ylab = expression(Pr(Y == y)),
       type = c("h", "g"), as.table = TRUE,
       strip = strip.custom(factor.levels = fl),
       sub = "Fonte: Elaborado pelo autor.")

@

Uma propriedade importante da distribuição Poisson é sua relação com a
distribuição Exponencial. Essa relação estabelece que se os tempos entre
a ocorrência de eventos se distribuem conforme modelo Exponencial de
parâmetro $\lambda$ a contagem de eventos em um intervalo de tempo $t$
tem distribuição Poisson com média $\lambda t$. A distribuição
\textit{Gamma-Count}, citada na tabela \ref{tab:distribuicoes}, estende
esta propriedade do processo adotando a distribuição Gama para os tempos
entre eventos tornando a distribuição da contagem decorrente mais
flexível \cite{Winkelmann1995, Zeviani2014}.

Outra propriedade que decorre da construção do modelo Poisson é sobre a
razão entre probabilidades sucessivas, $\frac{P(Y=y-1)}{P(Y=y)} =
\frac{y}{\lambda}$. Essa razão é linear em $y$ e tem sua taxa de
crescimento ou decrescimento como $\frac{1}{\lambda}$. Os modelos Katz e
COM-Poisson se baseiam na generalização da razão de probabilidades a fim
de flexibilizar a distribuição decorrente.

A utilização do modelo Poisson na análise de dados se dá por meio do
modelo de regressão Poisson. Seja $Y_i$ variáveis aleatórias
condicionalmente independentes, dados as covariáveis $X_i$,
$i=1,2,\cdots,n$. O modelo de regressão log-linear Poisson, sob a teoria
dos MLG's é definido como

\begin{equation}
  \label{eqn:reg-poisson}
  \begin{split}
    Y_i \mid & X_i \sim Poisson(\mu_i) \\
    &\log(\mu_i) = X_i\beta
  \end{split}
\end{equation}

\noindent
em que $\mu_i > 0$ é a média da variável aleatória $Y_i \mid X_i$ que é
calculada a partir do vetor $\beta \in \mathbb{R}^p$.

O processo de estimação do vetor $\beta$ é baseado na maximização da
verossimilhança que nas distribuições que pertencem à família
exponencial, os MLG's, é realizado via algoritmo de mínimos quadrados
ponderados iterativamente, ou, do inglês \textit{Iteractive Weighted
  Least Squares - IWLS} \cite{Nelder1972}.

\subsection{Estimação via Quase-Verossimilhança}
\label{cap02:estimacao-via-quase-verossimilhanca}

Em 1974 \citeauthoronline{Wedderburn1974} propôs uma forma de estimação
a partir de uma função biparamétrica, denoninada
quase-verossimilhança. Suponha que temos $y_i$ observações independentes
com esperanças $\mu_i$ e variâncias $V(\mu_i)$. A função de
quase-verossimilhança é é expressa como

\begin{equation}
  \label{eqn:quase-verossimilhanca}
  Q(\mu_i \mid y_i) = \int_y^{\mu_i} \frac{y_i - t}{\phi V(\mu_i)}dt
\end{equation}

Note na expressão \ref{eqn:quase-verossimilhanca} que a função de
quase-verossimilhança é definida a partir da especificação de $\mu_i$,
$V(\mu_i)$ e $\phi$. O processo de estimação via maximização dessa
função compartilha as mesmas estimativas para $\mu_i$, porém a dispersão
de $y_i$, $V(y_i) = \phi V(\mu_i)$ é corrigida pelo parâmetro adicional
$\phi$.

Assim os problemas com a fuga da suposição de equidispersão podem ser
superados quando a estimação por máxima quase-verossimilhança é
adotado. Porém um resultado dessa abordagem é que

\begin{equation}
  \label{eqn:quasi-informacao}
  -E\left ( \frac{\partial^2 Q(\mu \mid y)}{\partial \mu^2} \right) \leq
  -E\left ( \frac{\partial^2 \ell(\mu \mid y)}{\partial \mu^2} \right)
\end{equation}

\noindent
ou seja a informação a respeito de $\mu$ quando se conhece apenas $\phi$
e $V(\mu)$, a relação entre média e variância, é menor do que a
informação quando se conhece a distribuição da variável resposta, dada
pela log-verossimilhança $\ell(\mu \mid y)$. Além disso ressalta-se que,
de forma geral, não se recupera a distribuição de $Y$ somente com as
especificações de $\phi$ e $V(\mu)$.

Em modelos de regressão, definimos $g(\mu_i) = X\beta$ e $V(\mu_i)$ que
definem a função de quase-verossimilhança. Nessa abordagem são estimados
os parâmetros $\beta$ e $\phi$. A estimativa do vetor $\beta$ pode ser
obtidas pelo algoritmo \textit{IWLS}, usando as funções quase-escore e
matriz de quase-informação. Para o parâmetro $\phi$ um estimador usual é
o baseado na estatística $\chi^2$ de Pearson.

\begin{equation}
  \label{eqn:estimador-phi}
  \hat{\phi} = \frac{1}{n-p} \sum_{i=1}^n
                 \frac{(y_i - \hat{\mu_i})^2}{V(\hat{\mu_i})}
\end{equation}

\section{Modelo Binomial Negativo}
\label{cap02:binomneg}

Uma das principais alternativas paramétricas para dados de contagem
superdispersos é a adoção da distribuição Binomial Negativa. A função
massa de probabilidade da distribuição Binomial Negativa pode ser
deduzida de um processo hierárquico de efeitos aleatórios onde se assume
que

\begin{equation}
  \label{eqn:proc-binomneg}
  \begin{split}
    Y \mid & b \sim Poisson(b) \\
    & b \sim Gama(\mu, \phi)
  \end{split}
\end{equation}

\noindent
A função massa de probabilidade decorrente da estrutura descrita em
\ref{eqn:proc-binomneg} é deduzida integrando os efeitos aleatórios,
considere $f(y \mid b)$ como a função massa de probablidade da
distribuição Poisson (vide expressão em \ref{eqn:pmf-poisson}) e $g(b
\mid \mu, \phi)$ a função densidade da distribuição Gama \footnote{O
  desenvolvimento detalhado da integral pode ser visto em
  \citeonline[pág. 303-305]{Paula2013}. Obs.: A função densidade do
  modelo Gama está parametrizada para que $\mu$ represente a média da
  distribuição.}

\begin{equation}
  \label{eqn:proc-binomneg}
  \begin{split}
    Pr(Y = y \mid \mu,\phi) &= \int_0^\infty f(y \mid b)
       g(b \mid \mu,\phi) db\\
    &= \frac{\phi^\phi}{y!\mu^\phi\Gamma(\phi)}
       \int_0^\infty e^{-b(1 + \phi/\mu)} b^{y+\phi-1}db \\
    &= \frac{\Gamma(\phi + y)}{\Gamma(y+1)\Gamma(\phi)}
       \left ( \frac{\mu}{\mu + \phi} \right )^y
       \left ( \frac{\phi}{\mu + \phi} \right )^\phi
       \qquad y = 0, 1, 2, \cdots
  \end{split}
\end{equation}

\noindent
com $\mu >0$ e $\phi > 0$. Ressaltamos que esse é um caso particular de
um modelo de efeito aleatório cuja a integral tem solução analítica e
por consequência o modelo marginal tem forma fechada. Outro caso que se
baseia no mesmo princípio é o modelo \textit{Inverse Gaussian Poisson},
que como o nome sugere adota a distribuição Inversa Gaussiana para os
efeitos aleatórios. Na figura \ref{fig:distr-binomneg} são apresentadas
as distribuições Binomial Negativa para diferentes parâmetros $\phi$ em
comparação com a distribuição Poisson equivalente em locação. Note que
quanto menor o parâmetro $\phi$, maior a dispersão da distribuição. Isso
introduz uma propriedade importante desse modelo, para $\phi \rightarrow
\infty$ a distribuição reduz-se a Poisson.

<<distr-binomneg, fig.cap="Probabilidades pela distribuição Binomial Negativa para diferentes valores de $\\phi$ com $\\mu = 5$", fig.height=3.5, fig.width=7>>=

##-------------------------------------------
## Parametros da distribuição
mu <- 5
phis <- c("p1" = 1, "p2" = 5, "p3" = 30)
vars <- mu + (1/phis) * mu^2

##-------------------------------------------
## Calculando as probabilidades
y <- 0:15

## Binomial Negativa
py.bn <- sapply(phis, function(p) dnbinom(y, size = p, mu = mu))
da.bn <- as.data.frame(py.bn)
da.bn <- cbind(y, stack(da.bn))

## Poisson
py.po <- sapply(phis, function(p) dpois(y, lambda = mu))
da.po <- as.data.frame(py.po)
da.po <- cbind(y, stack(da.po))

##-------------------------------------------
## Objetos para grafico da lattice
fl <- substitute(
    expression(phi == p1, phi == p2, phi == p3),
    list(p1 = phis[1], p2 = phis[2], p3 = phis[3]))
cols <- trellis.par.get("superpose.line")$col[1:2]
yaxis <- pretty(da.po$values, n = 2)
ylim <- c(-0.08, max(da.po$values)*1.2)
key <- list(
    columns = 2,
    lines = list(lty = 1, col = cols),
    text = list(c("Poisson", "Binomial Negativa")))

##-------------------------------------------
## Grafico
xyplot(values ~ c(y - 0.15) | ind, data = da.po,
       type = c("h", "g"),
       xlab = "y", ylab = expression(P(Y == y)),
       ylim = ylim, xlim = extendrange(y),
       scales = list(y = list(at = yaxis)),
       layout = c(NA, 1),
       key = key,
       strip = strip.custom(factor.levels = fl),
       sub = "Fonte: Elaborado pelo autor.") +
    as.layer(xyplot(
        values ~ c(y + 0.15) | ind, data = da.bn,
        type = "h", col = cols[2]))
for(i in 1:3){
    trellis.focus("panel", i, 1, highlight=FALSE)
    grid::grid.text(label = sprintf("E[Y]:  %.1f\nV[Y]:  %.1f",
                                    mu, mu),
                    x = .62, y = 0.03,
                    default.units = "npc",
                    gp = grid::gpar(col = cols[1]),
                    just = c("left", "bottom"))
    grid::grid.text(label = sprintf("E[Y]:  %.1f\nV[Y]:  %.1f",
                                    mu, vars[i]),
                    x = .08, y = 0.03,
                    default.units = "npc",
                    gp = grid::gpar(col = cols[2]),
                    just = c("left", "bottom"))
}
trellis.unfocus()

@

Os momentos média e variância da distribuição Binomial Negativa são
expressos como $E(Y) = \mu$ e $V(Y) = \mu + \mu^2/\phi$. Note que pelas
expressões fica evidente a característica da Binomial Negativa de
acomodar somente superdispersão, pois $E(Y)$ é menor que $V(Y)$ para
qualquer $\phi$. Percebemos também quanto maior o parâmetro $\phi$ mais
$E(Y)$ se aproxima de $V(Y)$, e no limite $\phi \rightarrow \infty$,
$E(Y) = V(Y)$ fazendo com que a distribuição Binomial Negativa se reduza
a Poisson.

<<mv-binomneg, fig.cap="Relação Média e Variância na distribuição Binomial Negativa", fig.height=4, fig.width=4, fig.show="hide", results="asis">>=

##-------------------------------------------
## Parâmetros considerados
phi <- seq(0.5, 50, length.out = 50)
col <- rev(brewer.pal(n = 8, name = "RdBu"))
col <- colorRampPalette(colors = col)(length(phi))

##-------------------------------------------
## Etiquetas da legenda
labels <- substitute(
    expression(phi == p1, phi == p2, phi == p3),
    list(p1 = min(phi), p2 = median(phi), p3 = max(phi)))

##-------------------------------------------
## Gráfico

## Curva identidade representando a Poisson
par(mar = c(5.5, 4.2, 3, 3), las = 1)
curve(mu + 1*0,
      from = 0, to = 10, xname = "mu",
      ylab = expression(V(Y) == mu + mu^2~"/"~phi),
      xlab = expression(E(Y) == mu))
grid()
## Curvas da relação média e variância da Binomial Negativa
for (a in seq_along(phi)) {
    curve(mu + (mu^2)/phi[a],
          add = TRUE, xname = "mu", col = col[a], lwd = 2)
}
plotrix::color.legend(
    xl = 11, yb = 2.5, xr = 12, yt = 6.5,
    gradient = "y", align = "rb",
    legend = round(fivenum(phi)[c(1, 3, 5)]),
    rect.col = col)
mtext(text = expression(phi), side = 3, cex = 1.5,
      line = -4, at = 11.5)
fonte("Fonte: Elaborado pelo autor.")

wrapfigure()
@

A relação funcional entre média e variância é ilustrada na
figura \ref{fig:mv-binomneg} onde apesentamos as médias e variâncias
para $\mu$ entre 0 e 10 e $\phi$ entre 0 e 50. O comportamento dessa
relação proporciona um mairo flexibilidade à distribuição em acomodar
superdispersão, uma característica importante exibida nesta figura é que
para a Binomial Negativa se aproximar a Poisson em contagens altas o
$\phi$ deve ser extremamente grande.

O emprego do modelo Binomial Negativo em problemas se regressão ocorre
de maneira similar aos MLG's, com excessão de que a distribuição só
pertence a família exponencial de distribuições se o parâmetro $\phi$
for conhecido e assim o processo sofre algumas
alterações. Primeiramente, assim como na Poisson, definimos $g(\mu_i) =
X\beta$, comumente utiliza-se a função $g(\mu_i) =
\log(\mu_i)$. Desenvolvendo a log-verossimilhança e suas funções
derivadas, função escore e matriz de informação de Fisher chegamos que a
matriz de informação é bloco diagonal caracterizando a ortogonalidade
dos parâmetros $\beta$ de locação e $\phi$ de dispersão. Deste fato
decorre que a estimação dos parâmetros pode ser realizada em paralelo,
ou seja, estima-se o vetor $beta$ pelo método de \textit{IWLS} e
posteriormente o parâmetro $\phi$ pelo método de Newton-Raphson, faz-se
os dois procedimentos simultaneamente até a convengência dos parâmetros.

\section{Modelo COM-Poisson}
\label{cap02:compoisson}

A distribuição de probabilidades COM-Poisson foi proposta em 1962, em um
contexto de filas por \citeauthoronline{Conway1962} e generaliza a
Poisson em termos da razão de probabilidades sucessivas, como veremos
adiante. Seja $Y$ uma variável aleatória COM-Poisson, então sua função
massa de probabilidade é

\begin{equation}
  \label{eqn:pmf-compoisson}
  Pr(Y=y \mid \lambda, \nu) = \frac{\lambda^y}{(y!)^\nu Z(\lambda, \nu)}
  \qquad y = 0, 1, 2, \cdots
\end{equation}

\noindent
em que $\lambda > 0$, $\nu \geq 0$ e $Z(\lambda, \nu)$ é uma constante
de normalização, calculada para que de fato seja uma função massa de
probabilidade. $\sum_{i=1}^\infty Pr(Y = y) = 1$. $Z(\lambda, \nu)$ é
definida como se segue

\begin{equation}
  \label{eqn:constante-z}
  Z(\lambda, \nu) = \sum_{j=0}^\infty \frac{\lambda^j}{(j!)^\nu}
\end{equation}

O fato que torna a distribuição COM-Poisson mais flexível é a razão
entre probabilidades sucessivas

\begin{equation}
  \label{eqn:prob-ratio}
  \frac{Pr(Y=y-1)}{Pr(Y=y)} = \frac{y^\nu}{\lambda}
\end{equation}

\noindent
que se caracteriza não necessariamente linear em $y$, diferentemente da
Poisson, o que permite caudas mais pesadas ou mais magras à distribuição
\cite{Sellers2010}. Na figura \ref{fig:distr-compoisson} apresentamos as
dsitribuições COM-Poisson para diferentes valores de $\lambda$ e $\nu$
em contraste com as equivalentes, em locação, distribuições
Poisson. Nessa figura podemos apreciar a flexibilidade desse modelo,
pois i) contempla o caso de subdispersão mesmo em contagens baixas
($E(Y)=3$, painel a esquerda), a distribuição permite caudas pesadas e
consequentemente uma dispersão extra Poisson, ii) contempla subdisersão
mesmo em contagens altas, o que na Poisson teriamos variabilidade na mesma
magnitude, na COM-Poisson podemos ter caudas mais magras concentrando as
probabilidades em torno da média (painel a direita) e iii) tem como caso
particular a Poisson quando o parâmetro $\nu = 1$ (painel central).

<<distr-compoisson, fig.cap="Probabilidades pela distribuição COM-Poisson para diferentes parâmetros", fig.height=3.5, fig.width=7>>=

library(tccPackage)
##-------------------------------------------
## Parametros da distribuição
pars <- list("p1" = log(c(1.362, 0.4)),
             "p2" = log(c(8, 1)),
             "p3" = log(c(915, 2.5)))
mus <- sapply(pars, function(p) calc_mean_cmp(p[1], p[2], sumto = 50))
vars <- sapply(pars, function(p) calc_var_cmp(p[1], p[2], sumto = 50))

##-------------------------------------------
## Calculando as probabilidades
y <- 0:30

## COM-Poisson
py.co <- sapply(pars, function(p) dcmp(y, p[1], p[2], sumto = 50))
da.co <- as.data.frame(py.co)
da.co <- cbind(y, stack(da.co))

## Poisson
py.po <- sapply(mus, function(p) dpois(y, lambda = p))
da.po <- as.data.frame(py.po)
da.po <- cbind(y, stack(da.po))

##-------------------------------------------
## Objetos para grafico da lattice
l <- sapply(pars, function(p) exp(p[1]))
n <- sapply(pars, function(p) exp(p[2]))
fl <- substitute(
    expression(lambda == l1~","~nu == n1,
               lambda == l2~","~nu == n2,
               lambda == l3~","~nu == n3),
    list(l1 = l[1], l2 = l[2], l3 = l[3],
         n1 = n[1], n2 = n[2], n3 = n[3]))
cols <- trellis.par.get("superpose.line")$col[1:2]
yaxis <- pretty(da.po$values, n = 2)
ylim <- c(-0.1, max(da.po$values)*1.2)
key <- list(
    columns = 2,
    lines = list(lty = 1, col = cols),
    text = list(c("Poisson", "COM-Poisson")))

##-------------------------------------------
## Grafico
xyplot(values ~ c(y - 0.15) | ind, data = da.po,
       type = c("h", "g"),
       xlab = "y", ylab = expression(P(Y == y)),
       ylim = ylim, xlim = extendrange(y),
       scales = list(y = list(at = yaxis)),
       layout = c(NA, 1),
       key = key,
       strip = strip.custom(factor.levels = fl),
       sub = "Fonte: Elaborado pelo autor.") +
    as.layer(xyplot(
        values ~ c(y + 0.15) | ind, data = da.co,
        type = "h", col = cols[2]))
for(i in 1:3){
    trellis.focus("panel", i, 1, highlight=FALSE)
    grid::grid.text(label = sprintf("E[Y]:  %.1f\nV[Y]:  %.1f",
                                    mus[i], mus[i]),
                    x = .57, y = 0.03,
                    default.units = "npc",
                    gp = grid::gpar(col = cols[1]),
                    just = c("left", "bottom"))
grid::grid.text(label = sprintf("E[Y]:  %.1f\nV[Y]:  %.1f",
                                mus[i], vars[i]),
                x = .05, y = 0.03,
                default.units = "npc",
                gp = grid::gpar(col = cols[2]),
                just = c("left", "bottom"))
}
trellis.unfocus()

@

Uma das vantagens do modelo COM-Poisson é que possui, além da Poisson
quando $\nu = 1$ outros distribuições bem conhecidas como casos
particulares. Esses casos particulares ocorrem essencialmente devido a
forma assumida pela série infinita $Z(\lambda, \nu)$. Quando $\lambda =
1$, $Z(\lambda, \nu = 1) = e^\lambda$ e substituindo na expressão
\ref{eqn:pmf-compoisson} temos a distribuição Poisson resultante. Quando
$\nu \rightarrow \infty,\, Z(\lambda, \nu) \rightarrow 1+\lambda$ e a
distribuição COM-Poisson se aproxima de uma distribuição Bernoulli com
$P(Y=1)=\frac{\lambda}{1+\lambda}$. E quando $\nu = 0$ e $\lambda < 1$
$Z(\lambda, \nu)$ é uma soma geométrica que resulta em
$(1-\lambda)^{-1}$ e a expressão \ref{eqn:pmf-compoisson} se resume a
uma distribuição Geométrica com $P(Y=0)=(1-\lambda)$
\cite{Shmueli2005}. Os três respectivos casos particulares citados são
ilustrados na figura \ref{fig:casos-particulares}, onde determinamos os
parâmetros conforme reestrições para redução da distribuição.

<<casos-particulares, fig.cap="Exemplos de casos particulares da distribuição COM-Poisson", fig.height=3, fig.width=7>>=

library(tccPackage)
##-------------------------------------------
## Parametros da distribuição
pars <- list("p1" = log(c(5, 1)),
             "p2" = log(c(3, 20)),
             "p3" = log(c(0.5, 0)))

##-------------------------------------------
## Calculando as probabilidades
y <- list(y1 = 0:10, y2 = 0:2, y3 = 0:6)

## COM-Poisson
py.co <- sapply(1:3, function(p)
    dcmp(y[[p]], pars[[p]][1], pars[[p]][2], sumto = 50))
q <- sapply(py.co, length)

da <- data.frame(values = unlist(py.co),
                 y = unlist(y),
                 ind = rep(names(pars), q))

##-------------------------------------------
## Objetos para grafico da lattice
l <- sapply(pars, function(p) exp(p[1]))
n <- sapply(pars, function(p) exp(p[2]))
fl <- substitute(
    expression(lambda == l1~","~nu == n1,
               lambda == l2~","~nu == n2,
               lambda == l3~","~nu == n3),
    list(l1 = l[1], l2 = l[2], l3 = l[3],
         n1 = n[1], n2 = n[2], n3 = n[3]))

##-------------------------------------------
## Grafico
xyplot(values ~ y | ind, data = da,
       type = c("h", "g"),
       xlab = "y", ylab = expression(P(Y == y)),
       scales = list(relation = "free", rot = 0),
       layout = c(NA, 1),
       par.strip = list(lines = 2, col = "transparent"),
       sub = "Fonte: Elaborado pelo autor.")
distr <- c("Poisson", "Bernoulli", "Geométrica")
##-------------------------------------------
## http://stackoverflow.com/questions/33632344/strip-with-two-lines-title-r-lattice-plot
for (i in 1:3) {
    ## navigate to i-th strip
    vp <- paste0("plot_01.strip.", i, ".1.vp")
    grid::downViewport(vp)
    ## add first and second line of text
    grid::grid.text(distr[i], vjust = 0)
    grid::grid.text(eval(fl[[i+1]]), vjust = 1.1)
    ## navigate to top level
    grid::upViewport(0)
}

@

Um inconveniente desse modelo é que os momentos média e variância não
tem forma fechada. Sendo assim podem ser calculados a partir da
definição

$$
  E(Y) = \sum_{y = 0}^{\infty} y \cdot p(y) \qquad \qquad
  V(Y) = \sum_{y = 0}^{\infty} y^2 \cdot p(y) - E^2(Y)
$$

\citeonline{Shmueli2005}, a partir de uma aproximação para $Z(\lambda,
\nu)$, apresenta uma forma aproximada para os momentos da distribuição

\begin{equation}
  \label{eqn:cmp-mean-aprox}
  E(Y) \approx \lambda^{1/\nu} - \frac{\nu - 1}{2\nu} \qquad \qquad
  V(Y) \approx \frac{\lambda^{1/\nu}}{\nu}
\end{equation}

\noindent
os autores ressaltam que essa aproximação é satisfatória para $\nu \leq
1$ ou $\lambda > 10^\nu$. Na figura \ref{fig:mv-compoisson}
representamos de forma gráfica a relação média e variância aproximada
pelas expressões em \ref{eqn:cmp-mean-aprox}. Note que temos quase uma
relação linear entre média e variância, \citeonline{Sellers2010}
descrevem que essa pode ser aproximada por $\frac{1}{\nu}E(Y)$. Dessas
aproximações, bem como das visualizações em \ref{fig:distr-compoisson},
\ref{fig:casos-particulares} e \ref{fig:mv-compoisson} temos que o
parâmetros $\nu$, ou $\frac{1}{\nu}$, controla a precisão da distribuição,
sendo ela equidispersa $\mu = 1$, superdispersa quando $\nu < 1$
e subdispersa quando $\nu > 1$.

<<mv-compoisson, fig.cap="Relação Média e Variância na distribuição COM-Poisson", fig.height=4, fig.width=4, fig.show="hide", results="asis">>=

##-------------------------------------------
## Parâmetros considerados
nu <- seq(0.3, 4, length.out = 50)
col <- brewer.pal(n = 8, name = "RdBu")
col <- colorRampPalette(colors = col)(length(nu))

##-------------------------------------------
## Etiquetas da legenda
labels <- substitute(
    expression(nu == p1, nu == p2, nu == p3),
    list(p1 = min(nu), p2 = median(nu), p3 = max(nu)))

##-------------------------------------------
## Gráfico

par(mar = c(6.5, 5, 3, 3) + 0.1, las = 1)
## Curva identidade representando a Poisson
curve((1/1)*(mu + (1 - 1)/(2*1)), xlab = "", ylab = "",
      from = 0, to = 10, xname = "mu")
title(ylab = expression(V(X) == frac(nu*(E(X) + 1)-1, nu^2)),
      line = 2.5)
title(xlab = expression(E(X) == lambda^{1/nu} - frac(nu-1, 2*nu)),
      line = 4)
grid()

## Curvas da relação média e variância da Binomial Negativa
for (a in seq_along(nu)) {
    curve((1/nu[a])*(mu + (nu[a] - 1)/(2*nu[a])),
          add = TRUE, xname = "mu", col = col[a], lwd = 2)
}
plotrix::color.legend(
    xl = 11, yb = 2.5, xr = 12, yt = 6.5,
    gradient = "y", align = "rb",
    legend = round(fivenum(nu)[c(1, 3, 5)]),
    rect.col = col)
mtext(text = expression(nu), side = 3, cex = 1.5,
      line = -3.5, at = 11.5)
fonte("Fonte: Elaborado pelo autor.")

wrapfigure()

@

Embora o modelo COM-Poisson não tenha expressão fechada para a média da
distribuição pode-se utilizá-lo como modelo associado a distribuição
condicional da variável resposta de contagem. Isso é feito incorporando
um preditor linear em $\lambda$, que embora não representa a média está
associado com a locação da distribuição, ou seja, modela-se a média
indiretamente nessa abordagem. O modelo de regressão é definido com as
variáveis aleatórias condicionalmente independentes $Y_1, Y_2, \cdots,
Y_n$, dado o vetor de covariáveis $X_i = (x_{i1}, x_{i2}, \cdots,
x_{ip})$ seguindo um modelo COM-Poisson de parâmetros $\lambda_i =
e^{X_i\beta}$, $i = 1, 2, \cdots, n$ e $\nu$ comum a todas as
observações. Sob a notação de MLG's, temos em \ref{eqn:reg-poisson} o
modelo devidamente formulado

\begin{equation}
  \label{eqn:reg-compoisson}
  \begin{split}
    Y_i \mid & X_i \sim \textit{COM-Poisson}(\lambda_i, \nu) \\
    &\eta(E(Y_i \mid X_i)) = \log(\lambda_i) = X_i\beta
  \end{split}
\end{equation}

O algoritmo para estimação do conjunto de parâmetros $\Theta = (\nu,
\beta)$ do modelo é baseado na maximização da log-verossimilhança, que
decorrente da especificação em \ref{eqn:reg-compoisson} é

\begin{equation}
  \label{eqn:loglik-compoisson}
  \ell(\nu, \beta \mid \underline{y}) = \sum_i^n y_i \log(\lambda_i) -
  \nu \sum_i^n \log(y!) - \sum_i^n \log(Z(\lambda_i, \nu))
\end{equation}

\noindent
e então as estimativas de máxima verossimilhança são

$$
\hat{\Theta} = (\hat{\nu}, \hat{\beta}) =
\underset{(\nu,\,\beta)}{\textrm{arg max }} \ell(\nu, \beta \mid
\underline{y})
$$

<<constante-z, fig.cap="Convergência da constante de normalização da COM-Poisson para diferentes conjuntos de parâmetros", fig.height=3, fig.width=7>>=

##-------------------------------------------
## Calcula Z para um c(lambda, phi)
funZ <- function(lambda, nu, maxit = 500, tol = 1e-5) {
    z <- rep(NA, maxit)
    j = 1
    ##
    z[j] <- exp(j * log(lambda) - nu * lfactorial(j))
    ##
    while (abs(z[j] - 0) > tol && j <= maxit) {
        j = j + 1
        z[j] <- exp(j * log(lambda) - nu * lfactorial(j))
    }
    return(cbind("j" = 0:j, "z" = c(1, z[!is.na(z)])))
}

##-------------------------------------------
## Parametros da distribuição
pars <- list("p1" = c(1.362, 0.4), "p2" = c(8, 1), "p3" = c(915, 2.5))

##-------------------------------------------
## Calculando as componentes das constantes
zs <- sapply(pars, function(p) funZ(p[1], p[2]), simplify = FALSE)
da <- plyr::ldply(zs)

##-------------------------------------------
## Objetos para grafico da lattice
l <- sapply(pars, function(p) p[1])
n <- sapply(pars, function(p) p[2])
fl <- substitute(
    expression(lambda == l1~","~nu == n1,
               lambda == l2~","~nu == n2,
               lambda == l3~","~nu == n3),
    list(l1 = l[1], l2 = l[2], l3 = l[3],
         n1 = n[1], n2 = n[2], n3 = n[3]))

##-------------------------------------------
## Gráfico
xyplot(z ~ j | .id, data = da,
       type = c("b", "g"), pch = 19,
       scales = "free",
       ylab = list(
           expression(frac(lambda^j, "(j!)"^nu)),
           rot = 0),
       strip = strip.custom(factor.levels = fl),
       sub = "Fonte: Elaborado pelo autor.")

@

Note que nessa maximização a constante de normalização $Z(\lambda,
\nu)$, conforme definida em \ref{eqn:constante-z} é calcula para cada
indivíduo o que potencialmente torna o processo de estimação lento. Uma
ilustração do número de incrementos considerados para cálculo da
constante $Z(\lambda, \nu)$ é apresentado na figura
\ref{fig:constante-z}, neste ilustração foram utilizados os mesmos
parâmetros definidos em \ref{fig:distr-compoisson} e note que o número
de incrementos considerados para convergência \footnote{Adotou-se como
  critério de convergência a iteração $j$ tal que $\lambda^j/(j!)^\nu <
  0,00001$}. de $Z(\lambda, \nu)$ foram \Sexpr{c(table(da[, ".id"]))} nos
primeiro, segundo e terceiro painéis respectivamente.

Detalhes computacionais do algoritmo de maximização e manipulações
algébricas para eficiência na avaliação da log-verossimilhança no modelo
COM-Poisson são discutidos na seção \ref{cap03:metodos}.

\section{Modelos para excesso de zeros}
\label{cap02:zeros}

Problemas com excesso de zeros são comuns em dados de
contagem. Caracteriza-se como excesso de zeros casos em que a quantidade
observada de contagens nulas supera substancialmente aquela esperada
pelo modelo de contagem adotado, no caso do modelo Poisson
$e^{-\lambda}$.

As contagens nulas que geram o excesso de zeros podem ser decorridas de
duas formas distintas. A primeira denominamos de zeros estruturais,
quando a ocorrência de zero se dá pela ausência de determinada
característica na população e a segunda, que denominamos zeros amostrais
ocorre segundo um processo gerador de dados de contagem (e.g processo
Poisson). Assim, de forma geral temos dois processos geradores de dados
atuantes na geração de uma variável aleatória de contagem com excessivos
zeros.

Em geral, quando dados de contagem apresentam excessos de valores zero
também apresentarão subdispersão. Todavia, essa dispersão pode ser
exclusivamente devido ao excesso de zeros e assim os modelos
alternativos já apresentados não terão um bom desempenho. Uma ilustração
deste fato é ilustrada pela figura \ref{fig:ilustra-zeros}, em que
simulamos um conjunto de dados com excesso de ajustamos um modelo
COM-Poisson. Note que em ambos os casos o modelo se ajustou
adequadamente, indicando os excessos de zeros devem ser abordados de
forma diferente.

<<ilustra-zeros, fig.cap="Ilustração de dados de contagem com excesso de zeros", fig.height=3, fig.width=5>>=

##-------------------------------------------
## Simula os dados
set.seed(20124689)
n <- 1000

lambda <- 2; pi <- 0.9
y1 <- sapply(rbinom(n, 1, pi), function(x) {
    ifelse(x == 0, 0, rpois(1, lambda))
})

lambda <- 5; pi <- 0.85
y2 <- sapply(rbinom(n, 1, pi), function(x) {
    ifelse(x == 0, 0, rpois(1, lambda))
})

##-------------------------------------------
## Estimando as probabilidades
library(tccPackage)

sim <- list("s1" = as.integer(y1), "s2" = as.integer(y2))
probs <- sapply(sim, function(y) {
    yu <- 0:max(y)
    ##-------------------------------------------
    m0 <- glm(y ~ 1, family = poisson)
    py_pois <- dpois(yu, exp(m0$coef))
    ##-------------------------------------------
    m1 <- cmp(y ~ 1, data = data.frame(y = y), sumto = 40)
    py_dcmp <- dcmp(yu, loglambda = m1@coef[-1],
                    phi = m1@coef[1], sumto = 40)
    ##-------------------------------------------
    py_real <- c(prop.table(table(y)))
    ##-------------------------------------------
    cbind(yu, py_real, py_pois, py_dcmp)
}, simplify = FALSE)
da <- plyr::ldply(probs, .id="caso")

##-------------------------------------------
## Objetos para grafico da lattice
ylim <- with(da, extendrange(c(0, max(py_real, py_dcmp))))
cols <- trellis.par.get("superpose.line")$col[1:2]
key <- list(
    columns = 2,
    lines = list(lty = 1, col = cols),
    text = list(c("Observado", "COM-Poisson")))

##-------------------------------------------
## Gráfico
xyplot(py_real ~ c(yu - 0.15) | caso, data = da,
       type = c("h", "g"),
       xlab = "y",
       ylab = expression(Pr(Y==y)),
       ylim = ylim,
       key = key,
       strip = strip.custom(factor.levels = paste("Simulação", 1:2)),
       sub = "Fonte: Elaborado pelo autor.") +
    as.layer(xyplot(
        py_dcmp ~ c(yu + 0.15) | caso, data = da,
        type = "h", col = cols[2]))

@


\citeonline[capítulo 7]{Hilbe2014} discute sobre a interpretação e
modelagem de dados de contagem com excesso de zeros. Para essa situação
temos ao menos duas abordagens i) os modelos de mistura
\cite{Lambert1992}, também chamados de inflacionados, em inglês
\textit{Zero Inflated Models} e ii) os modelos condicionais
\cite{Ridout1998}, também chamados de modelos de barreira, em inglês
\textit{Hurdle Models}. Neste trabalho somente a abordagem via modelos
condicionais será abordada. A função massa de probabilidade do modelo
Hurdle é

\begin{equation}
  \label{eqn:pmf-hurdle}
  Pr(Y = y \mid \pi, \Theta_c) =
    \begin{dcases*}
      \pi & \text{se } y = 0,\\
      (1 - \pi) \frac{Pr(Z = z \mid \Theta_c)}{1 - Pr(Z = 0 \mid
        \Theta_c)} & \text{se } y = 1, 2, \dots
    \end{dcases*}
\end{equation}

\noindent
em que $0<\pi<1$, representa a probabilidade de ocorrência de zeros e
$Pr(Z = z \mid \Theta_c)$ a função massa de probabilidade de uma
variável aleatória de contagem $Z$, como a Poisson ou a Binomial
Negativa.

Da especificação em \ref{eqn:pmf-hurdle}, os momentos média e variância
são obtidos facilmente usando as definições $E(Y) = \sum_{y=1}^\infty y
\cdot Pr(Y=y)$ e $V(Y) = \sum_{y=1}^\infty y^2 \cdot Pr(Y=y) - E^2(Y)$

$$
E(Y) = \frac{E(Z)(1-\pi)}{1-Pr(Z = 0)} \qquad
V(Y) = \frac{1-\pi}{1-Pr(Z = 0)} \left [ E(Z) \frac{(1-\pi)}{1-Pr(Z =
    0)} \right ]
$$

Para a inclusão de covariáveis, caracterizando um problema de regressão,
dado que o modelo tem dois processos atuantes devemos modelar ambos como
se segue

\begin{equation}
  \label{eqn:reg-hurdle}
  \log \left (\frac{\pi_i}{1-\pi_i} \right ) = G_i\gamma \qquad e \qquad
  \begin{matrix}
    Z_i \sim D(\mu_i, \phi) \\
    g(\mu_i) = X_i\beta
  \end{matrix}
\end{equation}

\noindent
com $i = 1, 2, \cdots, n$, $G_i$ e $X_i$ as covariáveis da i-ésima
observação consideradas para explicação da contagens nulas e não nulas
respectivamente, $D(\mu_i, \phi)$ uma distribuição de probabilidades
para considerada para as contagens não nulas que pode conter ou não um
parâmetro $\phi$ adicional, se Poisson $D(\mu_i, \phi)$ se resume a
$Poisson(\mu_i)$ e $g(\mu_i)$ uma função de ligação, nos casos Poisson e
Binomial Negativa considera-se $\log(\mu_i)$. O que está implícito na
formulação \ref{eqn:reg-hurdle} é que para a componente que explica a
geração de zeros está sendo considerada a distribuição Bernoulli de
parâmetro $\pi_i$, contudo pode-se utilizar distribuições censuradas a
direita no ponto $y=1$ para estimação desta probabilidade, como explicam
\citeonline{Zeileis2007}.

\section{Modelos de efeitos aleatórios}
\label{cap02:aleatorio}

Nas seções anteriores exploramos modelos que flexibilizam algumas
suposições do modelo Poisson. Basicamente pertimindo casos não
equidispersos e modelando conjuntamente um processo gerador de zeros
extra. Contudo uma suposição dos modelos de regressão para dados de
contagem vistos até aqui é que as variáveis aleatória $Y_1, Y_2, \cdots,
Y_n$ são condicionalmente indenpendentes, dado o vetor de
covariáveis. Porém não são raras as situações em que essa suposição não
se mostra adequada. \citeonline{Ribeiro2012} cita alguns exemplos:

\begin{itemize}
  \item as observações podem ser correlacionadas no espaço,
  \item as observações podem ser correlacionadas no tempo,
  \item interações complexas podem ser necessárias para modelar o efeito
    conjunto de algumas covariáveis,
  \item heterogeneidade entre indivíduos ou unidades podem não ser
    suficientemente descrita por covariáveis.
\end{itemize}

Nessas situações pode-se estender a classe de modelos de regressão
com a adição de efeitos aleatórios que incorporam variáveis não
observáveis (latentes) ao modelo, permitindo assim acomodar uma
variabilidade, que pode ser ou não estruturada, não prescrita pelo
modelo. De forma geral a especificação dos modelos de efeitos aleatórios
segue uma especificação hierárquica

\begin{equation}
  \label{eqn:reg-misto}
  \begin{split}
    Y_{ij} \mid b_{i},& X_{ij} \sim \textrm{D}(\mu_{ij}, \phi) \\
    g(&\mu_{ij}) =  X_{ij}\beta + Z_ib_i \\
    & b \sim \textrm{K}(\Theta_b)
  \end{split}
\end{equation}

\noindent
para $i = 1, 2, \cdots, m$ (grupos com efeitos aleatórios comuns) e $j =
1, 2, \cdots, n$ (observações) com D$(\mu_{ij}, \phi)$, uma distribuição
considerada para as variáveis resposta condicionalmente independentes,
$g(\mu_{ij})$ uma função de ligação conforme definada na teoria dos
MLG's, $X_{ij}$ e $Z_{i}$ as vetores conhecidos representando os efeitos
das covariáveis de interesse, $b_i$ uma quantidade aleatória provida de
uma distribuição K$(\Theta_b)$. Note que nesses modelos uma quantidade
aleatória é somada ao preditor linear, diferentemente dos modelos de
efeitos fixos e a partir desta quantidade é possível induzir um
comportamento correlato entre as observações.

Como temos duas quantidades aleatórias no modelo, $Y \mid X$ e $b$, a
verossimilhança para um modelo de efeito aleatório é dada integrando-se
os efeitos aleatórios

\begin{equation}
  \label{eqn:loglik-misto}
  \Ell(\beta, \phi, \Theta_b \mid \underline{y}) = \prod_{i=1}^m \int_{\R^q}
  \left ( \prod_{j=1}^{n_i} f_D(y_{ij}, \mu, b_i)\right ) \cdot f_K(b
  \mid \Theta_b) db_i
\end{equation}

Perceba que na avaliação da verossimilhança é necessário o cálculo de
$m$ integrais de dimensão $q$. Para muitos casos essa integral não tem
forma analítica sendo necessários métodos númericos de aproximação, que
são discutidos na seção \ref{cap03:metodos}. E as estimativas de máxima
verossimilhança são

$$
\hat{\Theta} = (\hat{\beta}, \hat{\Theta_b}) =
\underset{(\beta,\,\Theta_b)}{\textrm{arg max }} \log(\Ell(\beta, \phi,
\Theta_b \mid \underline{y}))
$$

\noindent
note que no processo de estimação dos modelos de efeitos aleatórios,
métodos numéricos são intensivamente utilizados, pois a cada iteração do
algoritmo de maximização da log-verossimilhança $m$ integrais de
dimensão $q$ são aproximadas, ou seja, métodos de aproximação de
integrais são utilizados concomitantemente ao método de maximização.

Em modelos de contagem de efeitos mistos é comum adotar como
distribuição para os efeitos aleatórios uma Normal $q$-variada com média
0 e matriz de variância e covariâncias $\Sigma$, ou seja, na
especificação \ref{eqn:reg-misto} K$(\Theta_b) = NMV_q(0, \Sigma)$. Para
estes casos os principais métodos de aproximação da integral tem
desempenhos melhores \cite{Bates2015}.

Como mencionado anteriormente modelos de efeitos aleatórios são
candidatos a modelagem de dados superdispersos. Quando não há uma
estrutura de delineamento experimental ou observacional pode-se incluir
efeitos aleatórios a nível de observação (e então $m=n$, ou seja, os
vetores $Y$ e $b$ tem mesma dimensão). Casos particulares de modelos de
efeitos aleatórios, onde o efeito aleatório é adiciona a nível de
observação são o modelo Binomial Negativo e o \textit{Inverse Gaussian
  Model}, em ambos os casos a integral, definida em
\ref{eqn:loglik-misto} tem solução analítica e consequentemente a
marginal em $Y$ forma fechada.
