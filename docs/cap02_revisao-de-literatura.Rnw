% ------------------------------------------------------------------------
% CAPÍTULO 2 - REVISÃO DE LITERATURA
% ------------------------------------------------------------------------

Métodos para inferência em dados de contagem estão bem aquém da
quantidade disponível para dados contínuos. Destaca-se o modelo
log-linear Poisson como o modelo mais utilizado quando se trata de dados
de contagem. Porém, não raramente os dados de contagens apresentam
variância superior ou inferior à sua média. Esses são os casos de super
ou subdispersão já enunciados no \autoref{cap:introducao} que,
quando ocorrem, inviabilizam o uso da distribuição Poisson.

Nos casos de falha da suposição de equidispersão, são alternativas os
métodos de estimação via quase-verossimilhança, estimação robusta dos
erros padrões (estimador ``sanduíche'') e estimação dos erros padrões
via reamostragem (``\textit{bootstrap}'') \cite{Hilbe2014}. Desses
métodos detalha-se, brevemente, somente o método de estimação via função
de quase-verossimilhança na
\autoref{cap02:estimacao-via-quase-verossimilhanca}.

No contexto paramétrico, pesquisas recentes trazem modelos bastante
flexíveis, veja \citeonline{Sellers2010, Zeviani2014, Lord2010}. No
\autoref{quad:distribuicoes}, são listadas as distribuições de
probabilidades consideradas por \citeonline{Winkelmann2008} e
\citeonline{Kokonendji2014}. Nota-se que a Poisson é a única das
distribuições listadas que contempla somente a característica de
equidispersão. Observa-se um conjunto maior de distribuições para os
casos de superdispersão com relação aos casos de subdispersão. Embora um
grande número de distribuições exista para modelar diferentes níveis de
dispersão, são raras as implementação computacionais em pacotes
estatísticos que as disponibilizam como alternativas para ajuste de
modelos de regressão.

%%----------------------------------------------------------------------
%% Tabela das distribuições para dados de contagem
\begin{quadro}
\centering
\caption{Distribuições de probabilidades para dados de contagem com
  indicação das características contempladas}
\label{quad:distribuicoes}
\begin{tabular}{lccc}
  \toprule
\multirow{2}{*}{Distribuição}       & \multicolumn{3}{c}{Contempla a característica de} \\
  \cline{2-4} \\[-0.3cm]
                                    & Equidispersão     & Superdispersão & Subdispersão \\[0.1cm]
  \hline
Poisson                             & \checkmark        &                &              \\
Binomial negativa                   & \checkmark        & \checkmark     &              \\
Poisson inversa-Gaussiana           & \checkmark        & \checkmark     &              \\
\textit{Compound Poisson}           & \checkmark        & \checkmark     &              \\
Poisson Generalizada                & \checkmark        & \checkmark     & \checkmark   \\
\textit{Gamma-Count}                & \checkmark        & \checkmark     & \checkmark   \\
COM-Poisson                         & \checkmark        & \checkmark     & \checkmark   \\
\textit{Double-Poisson}             & \checkmark        & \checkmark     & \checkmark   \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
  \small
\item Fonte: Elaborado pelo autor.
\end{tablenotes}
\end{quadro}
%%----------------------------------------------------------------------

Dos modelos paramétricos, o binomial negativo aparece em destaque com
implementações já consolidadas nos principais softwares e frequentes
aplicações nos casos de superdispersão. Na \autoref{cap02:binomneg},
detalhes da construção desses modelos são apresentados. Dos demais
modelos derivados das distribuições listadas no
\autoref{quad:distribuicoes}, este trabalho abordará somente o modelo
COM-Poisson, que é apresentado com detalhes na
\autoref{cap02:compoisson}.

Um outro fenômeno que é frequente em dados de contagem é a ocorrência
excessiva de zeros, quando comparada ao esperado pela distribuição
adotada. Esse fenômeno sugere a modelagem de dois processos geradores de
dados, o gerador de zeros extras e o gerador das contagens. Existem ao
menos duas abordagens pertinentes para estes casos que são os modelos de
mistura e os modelos condicionais. Na abordagem por modelos de mistura a
variável resposta é modelada como uma mistura de duas
distribuições. \citeonline{Lambert1992} apresenta uma mistura da
distribuição Bernoulli com uma distribuição de Poisson ou binomial
begativa. Considerando os modelos condicionais, também chamados de
modelos de barreira \cite{Ridout1998}, tem-se que a modelagem da
variável resposta é realizada em duas etapas. A primeira refere-se ao
processo gerador de contagens nulas e a segunda ao gerador de contagens
não nulas. Nesse trabalho, a modelagem de excesso de zeros é realizada
incluindo um componente de barreira. A \autoref{cap02:zeros} é destinada
a um breve detalhamento dessa abordagem.

Neste capítulo, apresenta-se também os modelos com inclusão de efeitos
aleatórios na \autoref{cap02:aleatorio}. Em análise de dados de contagem,
a inclusão desses efeitos permite acomodar variabilidade extra e
incorporar a estrutura amostral do problema, como em experimentos com
medidas repetidas ou longitudinais, experimentos em parcelas
subdivididas ou dados com grupos heterogêneos.

\section{Modelo Poisson}
\label{cap02:poisson}

A distribuição Poisson é uma das principais distribuições de
probabilidades discretas. Com suporte nos inteiros não negativos, uma
variável aleatória segue um modelo Poisson se sua função massa de
probabilidade for
\begin{equation}
  \label{eqn:pmf-poisson}
  \Pr(Y = y \mid \lambda) = \frac{\lambda^ye^{-\lambda}}{y!},
  \qquad y = 0, 1, 2, \ldots,
\end{equation}
em que $\lambda > 0$ representa a taxa de ocorrência do evento. Uma
particularidade já destacada dessa distribuição é que
$\text{E}(X) = \text{V}(X) = \lambda$. Isso torna a distribuição Poisson
bastante restritiva. Na \autoref{fig:distr-poisson}, são apresentadas as
distribuições Poisson para diferentes parâmetros. Note que, devido a
propriedade $\text{E}(X) = \text{V}(X)$, contagens de médias maiores
também tem probabilidades mais dispersas.

<<distr-poisson, fig.height=3.3, fig.width=6.7, fig.cap="Probabilidades pela distribuição Poisson para diferentes parâmetros.">>=

lambdas <- c("p1" = 3, "p2" = 8, "p3" = 15)
y <- 0:30
py <- sapply(lambdas, function(p) dpois(y, p))
da <- as.data.frame(py)
da <- cbind(y, stack(da))

fl <- substitute(expression(
    lambda == p1, lambda == p2, lambda == p3),
    list(p1 = lambdas[1], p2 = lambdas[2], p3 = lambdas[3]))

xyplot(values ~ y | factor(ind), data = da,
       layout = c(NA, 1),
       ylab = expression(Pr(Y == y)),
       type = c("h", "g"), as.table = TRUE,
       strip = strip.custom(factor.levels = fl),
       sub = "Fonte: Elaborado pelo autor.")

@

Uma propriedade importante da distribuição Poisson é sua relação com a
distribuição exponencial. Essa relação estabelece que se os tempos entre
as ocorrências dos eventos se distribuem conforme modelo exponencial de
parâmetro $\lambda$, a contagem de eventos em um intervalo de tempo $t$
tem distribuição Poisson com média $\lambda t$. A distribuição
\textit{Gamma-Count}, citada no \autoref{quad:distribuicoes}, estende
essa propriedade adotando a distribuição gama para o tempo entre
eventos, tornando a distribuição da contagem decorrente mais flexível
\cite{Winkelmann1995, Zeviani2014}.

Outra propriedade que decorre da construção do modelo Poisson é sobre a
razão entre probabilidades sucessivas,
$\frac{\Pr(Y=y-1)}{\Pr(Y=y)} = \frac{y}{\lambda}$. Essa razão é linear
em $y$ e tem sua taxa de variação instantânea igual a
$\frac{1}{\lambda}$. Os modelos COM-Poisson se baseiam na generalização
dessa razão de probabilidades para flexibilizar a distribuição de
probabilidades.

A utilização do modelo Poisson na análise de dados se dá por meio do
modelo de regressão Poisson. Sejam $Y_1, Y_2, \ldots, Y_n$ variáveis
aleatórias condicionalmente independentes, dado o vetor de covariáveis
$\bm{x}_i^\top=(x_{i1},x_{i2},\ldots,x_{ip})$. O modelo de regressão
log-linear Poisson, sob a teoria dos MLG's, é definido como
\begin{equation}
  \label{eqn:reg-poisson}
  \begin{split}
    Y_i \mid & \bm{x}_i \sim \textrm{Poisson}(\mu_i) \\
    &\log(\mu_i) = \bm{x}_i^\top\bm{\beta},
  \end{split}
\end{equation}
em que $\mu_i > 0$ é a média da variável aleatória $Y_i$ condicionada ao
vetor de covariáveis $\bm{x}_i^\top$, que é calculada a partir do
vetor $\bm{\beta} \in \mathbb{R}^p$.

O processo de estimação dos parâmetros $\bm{\beta}$ é baseado na
maximização da função de verossimilhança que, nas distribuições
pertencentes à família exponencial, é realizada via algoritmo de mínimos
quadrados ponderados iterativamente, ou, do inglês \textit{Iteractive
  Weighted Least Squares - IWLS} \cite{Nelder1972, McCullagh1989}.

\subsection{Estimação via quase-Verossimilhança}
\label{cap02:estimacao-via-quase-verossimilhanca}

\citeonline{Wedderburn1974} propôs uma forma de estimação a partir de
uma função de quase-verossimilhança. Suponha $Y_1, Y_2, \ldots, Y_n$
variáveis aleatórias independentes com $\text{E}(Y_i) = \mu_i$ e função
de variância $V(\mu_i)$, em que $V$ é uma função positiva e conhecida. A
função de quase-verossimilhança é expressa como
\begin{equation}
  \label{eqn:quase-verossimilhanca}
  Q(\mu_i \mid y_i) = \int_{y_i}^{\mu_i}
  \frac{y_i - \mu_i}{\sigma^2 V(\mu_i)}d\mu_i.
\end{equation}

Na \autoref{eqn:quase-verossimilhanca}, a função de
quase-verossimilhança é definida a partir da especificação de $\mu_i$,
$V(\mu_i)$ e $\sigma^2$. O processo de estimação via maximização dessa
função compartilha, do método baseado na maximazação da função de
verossimilhança, as mesmas estimativas para $\mu_i$, porém a dispersão
de $y_i$ é corrigida pelo parâmetro adicional $\sigma^2$,
$\text{V}(Y_i) = \sigma^2 V(\mu_i)$.

Com a adição desse parâmetro de dispersão $\sigma^2$, relaxa-se a
suposição de equidispersão. Porém, um resultado dessa abordagem é que
\begin{equation}
  \label{eqn:quasi-informacao}
  -\text{E}\left ( \frac{\partial^2 Q(\mu \mid y)}{\partial \mu^2}
    \right) \leq
  -\text{E}\left ( \frac{\partial^2 \ell(\mu \mid y)}{\partial \mu^2}
    \right),
\end{equation}
ou seja, a informação a respeito de $\mu$ quando se descreve apenas
$\sigma^2$ e $V(\mu)$, a relação média--variância, é menor do que a
informação quando se descreve a distribuição da variável resposta, dada
pela log-verossimilhança $\ell(\mu \mid y)$. Além disso, ressalta-se
que, a menos de casos particulares, não é possível descrever uma
distribuição de probabilidades para $Y$ somente com as especificações de
$\sigma^2$ e $V(\mu)$.

Em modelos de regressão, $g(\mu_i) = \bm{x}_i^\top\beta$ e $V(\mu_i)$
definem a função de quase-verossimilhança. Nessa abordagem, são
estimados os parâmetros $\bm{\beta}$ e $\sigma^2$. A estimação do vetor
$\bm{\beta}$ é realizada pelo algoritmo \textit{IWLS}. Usando o vetor
quase-escore, derivadas de primeira ordem da função $Q(\mu_i \mid y_i)$
em relaçao à $\bm{\beta}$, e matriz de quase-informação, derivadas de
segunda ordem, chega-se ao mesmo algoritmo de estimação dado no caso
Poisson, que não depende de $\sigma^2$. O parâmetro $\sigma^2$ é
estimado separadamente, pós estimação dos $\beta$'s. Um estimador usual
é o baseado na estatística $\chi^2$ de Pearson
\begin{equation}
  \label{eqn:estimador-theta}
  \hat{\sigma^2} = \frac{1}{n-p} \sum_{i=1}^n
    \frac{(y_i - \hat{\mu_i})^2}{V(\hat{\mu_i})}.
\end{equation}

\section{Modelo Binomial Negativo}
\label{cap02:binomneg}

Uma das principais distribuições paramétricas para dados de contagem
superdispersos é a binomial negativa. A função massa de probabilidade de
uma variável aleatória com distribuição binomial negativa pode ser
deduzida de um processo hierárquico em que se assume
\begin{equation}
  \label{eqn:proc-binomneg}
  \begin{gathered}
    Y \mid b \sim \textrm{Poisson}(b) \\
     b \sim \textrm{Gama}(\mu, \theta)
  \end{gathered}
\end{equation}

<<distr-binomneg, fig.height=3.4, fig.width=6.7, fig.cap="Probabilidades pela distribuição Binomial Negativa para diferentes níveis de dispersão, fixando a média em 5.">>=

##-------------------------------------------
## Parametros da distribuição
mu <- 5
thetas <- c("p1" = 1, "p2" = 5, "p3" = 30)
vars <- mu + (1/thetas) * mu^2

##-------------------------------------------
## Calculando as probabilidades
y <- 0:15

## Binomial Negativa
py.bn <- sapply(thetas, function(p) dnbinom(y, size = p, mu = mu))
da.bn <- as.data.frame(py.bn)
da.bn <- cbind(y, stack(da.bn))

## Poisson
py.po <- sapply(thetas, function(p) dpois(y, lambda = mu))
da.po <- as.data.frame(py.po)
da.po <- cbind(y, stack(da.po))

##-------------------------------------------
## Objetos para grafico da lattice
fl <- substitute(
    expression(theta == p1, theta == p2, theta == p3),
    list(p1 = thetas[1], p2 = thetas[2], p3 = thetas[3]))
cols <- trellis.par.get("superpose.line")$col[1:2]
yaxis <- pretty(da.po$values, n = 2)
ylim <- c(-0.08, max(da.po$values)*1.2)
key <- list(
    columns = 2,
    lines = list(lty = 1, col = cols),
    text = list(c("Poisson", "Binomial Negativa")))

##-------------------------------------------
## Grafico
xyplot(values ~ c(y - 0.15) | ind, data = da.po,
       type = c("h", "g"),
       xlab = "y", ylab = expression(Pr(Y == y)),
       ylim = ylim, xlim = extendrange(y),
       scales = list(y = list(at = yaxis)),
       layout = c(NA, 1),
       key = key,
       strip = strip.custom(factor.levels = fl),
       sub = "Fonte: Elaborado pelo autor.") +
    as.layer(xyplot(
        values ~ c(y + 0.15) | ind, data = da.bn,
        type = "h", col = cols[2]))
for(i in 1:3){
    trellis.focus("panel", i, 1, highlight=FALSE)
    grid::grid.text(label = sprintf("E(Y):  %.1f\nV(Y):  %.1f",
                                    mu, mu),
                    x = .62, y = 0.03,
                    default.units = "npc",
                    gp = grid::gpar(col = cols[1]),
                    just = c("left", "bottom"))
    grid::grid.text(label = sprintf("E(Y):  %.1f\nV(Y):  %.1f",
                                    mu, vars[i]),
                    x = .08, y = 0.03,
                    default.units = "npc",
                    gp = grid::gpar(col = cols[2]),
                    just = c("left", "bottom"))
}
trellis.unfocus()

@

A função massa de probabilidade de $Y$, decorrente da estrutura descrita
na \autoref{eqn:proc-binomneg} é deduzida integrando os efeitos
aleatórios.  Considere $f(y \mid b)$ como a função massa de
probabilidade da distribuição Poisson (vide \autoref{eqn:pmf-poisson}) e
$g(b \mid \mu, \phi)$ a função densidade da distribuição Gama\footnote{O
  desenvolvimento detalhado da integral pode ser visto em
  \citeonline[pág. 303-305]{Paula2013}. Obs.: A função densidade do
  modelo Gama está parametrizada para que $\mu$ represente a média da
  distribuição.}
\begin{equation}
  \label{eqn:proc-binomneg}
  \begin{split}
    \Pr(Y = y \mid \mu,\theta) &= \int_0^\infty f(y \mid b)
       g(b \mid \mu,\theta) db\\
    &= \frac{\theta^\theta}{y!\mu^\theta\Gamma(\theta)}
       \int_0^\infty e^{-b(1 + \theta/\mu)} b^{y+\theta-1}db \\
    &= \frac{\Gamma(\theta + y)}{\Gamma(y+1)\Gamma(\theta)}
       \left ( \frac{\mu}{\mu + \theta} \right )^y
       \left ( \frac{\theta}{\mu + \theta} \right )^\theta,
  \end{split}
\end{equation}
com $\mu >0$ e $\theta > 0$. Esse é um caso particular de um modelo de
efeito aleatório, cuja integral tem solução analítica e, por
consequência, o modelo marginal tem forma fechada. Outro caso que se
baseia no mesmo princípio é o modelo Poisson inverso-Gaussiano, que,
como o nome sugere, adota a distribuição inversa-Gaussiana para os
efeitos aleatórios. Na \autoref{fig:distr-binomneg}, são apresentadas as
distribuições binomial negativa para diferentes parâmetros $\theta$ em
comparação com a distribuição Poisson, equivalente em locação. Note que,
quanto menor o parâmetro $\theta$, maior a dispersão da
distribuição. Isso introduz uma propriedade importante desse modelo,
para $\theta \to \infty$ a distribuição reduz-se a Poisson.

<<mv-binomneg, fig.height=4, fig.width=4, fig.cap="Relação média--variância na distribuição Binomial Negativa.">>=

##-------------------------------------------
## Parâmetros considerados
theta <- seq(0.5, 50, length.out = 50)
col <- rev(brewer.pal(n = 8, name = "RdBu"))
col <- colorRampPalette(colors = col)(length(theta))

##-------------------------------------------
## Etiquetas da legenda
labels <- substitute(
    expression(theta == p1, theta == p2, theta == p3),
    list(p1 = min(theta), p2 = median(theta), p3 = max(theta)))

##-------------------------------------------
## Gráfico

## Curva identidade representando a Poisson
par(mar = c(5.5, 4.2, 3, 3), las = 1)
curve(mu + 1*0,
      from = 0, to = 10, xname = "mu",
      ylab = expression(V(Y) == mu + mu^2~"/"~theta),
      xlab = expression(E(Y) == mu))
grid()
## Curvas da relação média e variância da Binomial Negativa
for (a in seq_along(theta)) {
    curve(mu + (mu^2)/theta[a],
          add = TRUE, xname = "mu", col = col[a], lwd = 2)
}
plotrix::color.legend(
    xl = 11, yb = 2.5, xr = 12, yt = 6.5,
    gradient = "y", align = "rb",
    legend = round(fivenum(theta)[c(1, 3, 5)]),
    rect.col = col)
mtext(text = expression(theta), side = 3, cex = 1.3,
      line = -4, at = 11.5)
fonte("Fonte: Elaborado pelo autor.", cex = 0.95)

@

Os momentos média e variância da distribuição binomial negativa são
dados por $\text{E}(Y) = \mu$ e $\text{V}(Y) = \mu + \mu^2/\sigma^2$.
Pelas expressões fica evidente a característica da binomial negativa
modelar somente superdispersão, pois $\text{E}(Y)$ é menor que
$\text{V}(Y)$ para qualquer $\sigma^2$. No caso limite, quando
$\sigma^2 \rightarrow \infty$, tem-se que $\text{E}(Y) = \text{V}(Y)$
fazendo com que a distribuição binomial begativa se reduza à Poisson.

A relação funcional entre média e variância é ilustrada na
\autoref{fig:mv-binomneg} em que são apresentadas as médias e variâncias
para $\mu$, entre 0 e 10, e $\theta$, entre 0 e 50. O comportamento
dessa relação proporciona uma maior flexibilidade à distribuição em
acomodar superdispersão. Uma característica importante exibida nessa
figura é que para a binomial negativa se aproximar da Poisson em médias
altas o $\theta$ deve ser extremamente grande.

O emprego do modelo binomial negativo em problemas de regressão ocorre
de maneira similar aos MLG's, com exceção de que a distribuição só
pertence à família exponencial se o parâmetro $\theta$ for fixado e
assim o processo sofre algumas alterações. Primeiramente, define-se
$g(\mu_i) = \bm{x}_i^\top\beta$, comumente utiliza-se a função
$g(\mu_i) = \log(\mu_i)$. A partir da log-verossimilhança e suas
derivadas, vetor escore e matriz de informação de Fisher, mostra-se que
matriz de informação é bloco diagonal caracterizando a ortogonalidade
dos parâmetros $\bm{\beta}$ de locação e $\theta$ de dispersão. Desse
fato, decorre que a estimação dos parâmetros pode ser realizada em
paralelo, ou seja, estima-se o vetor $\bm{\beta}$ pelo algoritmo
\textit{IWLS} e posteriormente o parâmetro $\theta$ pelo método de
Newton-Raphson.  Os dois procedimentos são realizados simultaneamente
até a convergência das estimativas.

\section{Modelo COM-Poisson}
\label{cap02:compoisson}

A distribuição de probabilidades COM-Poisson foi proposta por
\citeonline{Conway1962}, em um contexto de filas, e generaliza a Poisson
em termos da razão de probabilidades sucessivas, como será visto
adiante. Seja $Y$ uma variável aleatória COM-Poisson então sua função
massa de probabilidade é
\begin{equation}
  \label{eqn:pmf-compoisson}
  \Pr(Y=y \mid \lambda, \nu) = \frac{\lambda^y}{(y!)^\nu Z(\lambda, \nu)},
  \qquad y = 0, 1, 2, \ldots,
\end{equation}
em que $\lambda > 0$, $\nu \geq 0$ e $Z(\lambda, \nu)$ é uma constante
de normalização, calculada para que de fato a \autoref{eqn:pmf-compoisson}
seja uma função massa de probabilidade ($\sum_{i=0}^\infty \Pr(Y = i) =
1$). A função $Z(\lambda, \nu)$ é definida como se segue
\begin{equation}
  \label{eqn:constante-z}
  Z(\lambda, \nu) = \sum_{j=0}^\infty \frac{\lambda^j}{(j!)^\nu}.
\end{equation}

O fato que torna a distribuição COM-Poisson mais flexível é a razão
entre probabilidades sucessivas
\begin{equation}
  \label{eqn:prob-ratio}
  \frac{\Pr(Y=y-1)}{\Pr(Y=y)} = \frac{y^\nu}{\lambda},
\end{equation}
que se caracteriza não, necessariamente, linear em $y$, diferentemente
da Poisson, o que permite caudas mais pesadas ou mais leves à
distribuição \cite{Sellers2010}. Na \autoref{fig:distr-compoisson}, são
apresentadas as distribuições COM-Poisson para diferentes valores de
$\lambda$ e $\nu$, em contraste com as equivalentes, em locação,
distribuições Poisson. Nessa figura, pode-se ver a flexibilidade desse
modelo, pois i) contempla o caso de subdispersão mesmo em contagens
baixas ($\text{E}(Y)=3$, painel a esquerda), a distribuição permite caudas
pesadas e consequentemente uma dispersão extra Poisson; ii) contempla
subdispersão mesmo em contagens altas, onde na Poisson tem-se
variabilidade na mesma magnitude, na COM-Poisson pode-se ter caudas mais
leves concentrando as probabilidades em torno da média (painel a
direita); e iii) tem como caso particular a Poisson quando o parâmetro
$\nu = 1$ (painel central).

<<distr-compoisson, fig.height=3.4, fig.width=6.7, fig.cap="Probabilidades pela distribuição COM-Poisson para diferentes parâmetros.">>=

library(cmpreg)
##-------------------------------------------
## Parametros da distribuição
pars <- list("p1" = log(c(1.362, 0.4)),
             "p2" = log(c(8, 1)),
             "p3" = log(c(915, 2.5)))
mus <- sapply(pars, function(p) calc_mean_cmp(p[1], p[2], sumto = 50))
vars <- sapply(pars, function(p) calc_var_cmp(p[1], p[2], sumto = 50))

##-------------------------------------------
## Calculando as probabilidades
y <- 0:30

## COM-Poisson
py.co <- sapply(pars, function(p) dcmp(y, p[1], p[2], sumto = 50))
da.co <- as.data.frame(py.co)
da.co <- cbind(y, stack(da.co))

## Poisson
py.po <- sapply(mus, function(p) dpois(y, lambda = p))
da.po <- as.data.frame(py.po)
da.po <- cbind(y, stack(da.po))

##-------------------------------------------
## Objetos para grafico da lattice
l <- sapply(pars, function(p) exp(p[1]))
n <- sapply(pars, function(p) exp(p[2]))
fl <- substitute(
    expression(lambda == l1~","~nu == n1,
               lambda == l2~","~nu == n2,
               lambda == l3~","~nu == n3),
    list(l1 = l[1], l2 = l[2], l3 = l[3],
         n1 = n[1], n2 = n[2], n3 = n[3]))
cols <- trellis.par.get("superpose.line")$col[1:2]
yaxis <- pretty(da.po$values, n = 2)
ylim <- c(-0.1, max(da.po$values)*1.2)
key <- list(
    columns = 2,
    lines = list(lty = 1, col = cols),
    text = list(c("Poisson", "COM-Poisson")))

##-------------------------------------------
## Grafico
xyplot(values ~ c(y - 0.15) | ind, data = da.po,
       type = c("h", "g"),
       xlab = "y", ylab = expression(Pr(Y == y)),
       ylim = ylim, xlim = extendrange(y),
       scales = list(y = list(at = yaxis)),
       layout = c(NA, 1),
       key = key,
       strip = strip.custom(factor.levels = fl),
       sub = "Fonte: Elaborado pelo autor.") +
    as.layer(xyplot(
        values ~ c(y + 0.15) | ind, data = da.co,
        type = "h", col = cols[2]))
for(i in 1:3){
    trellis.focus("panel", i, 1, highlight=FALSE)
    grid::grid.text(label = sprintf("E(Y):  %.1f\nV(Y):  %.1f",
                                    mus[i], mus[i]),
                    x = .57, y = 0.03,
                    default.units = "npc",
                    gp = grid::gpar(col = cols[1]),
                    just = c("left", "bottom"))
grid::grid.text(label = sprintf("E(Y):  %.1f\nV(Y):  %.1f",
                                mus[i], vars[i]),
                x = .05, y = 0.03,
                default.units = "npc",
                gp = grid::gpar(col = cols[2]),
                just = c("left", "bottom"))
}
trellis.unfocus()

@

Uma das vantagens do modelo COM-Poisson é que possui, além da Poisson
quando $\nu = 1$, outras distribuições bem conhecidas como casos
particulares. Esses casos particulares ocorrem essencialmente devido à
forma assumida pela série infinita $Z(\lambda, \nu)$. Quando $\nu = 1$,
$Z(\lambda, \nu = 1) = e^\lambda$ e substituindo na
\autoref{eqn:pmf-compoisson}, tem-se a distribuição Poisson
resultante. Quando $\nu \rightarrow \infty,\, Z(\lambda, \nu)
\rightarrow 1+\lambda$ e a distribuição COM-Poisson se aproxima de uma
distribuição Bernoulli com $P(Y=1)=\frac{\lambda}{1+\lambda}$. E quando
$\nu = 0$ e $\lambda < 1$ $Z(\lambda, \nu)$ é uma soma geométrica que
resulta em $(1-\lambda)^{-1}$ e a \autoref{eqn:pmf-compoisson} se resume
a uma distribuição geométrica com $P(Y=0)=(1-\lambda)$
\cite{Shmueli2005}. Os três casos particulares citados são ilustrados na
\autoref{fig:casos-particulares}, onde os parâmetros foram escolhidos
conforme restrições para redução da distribuição.

<<casos-particulares, fig.height=3, fig.width=7, fig.cap="Exemplos de casos particulares da distribuição COM-Poisson.">>=

library(cmpreg)
##-------------------------------------------
## Parametros da distribuição
pars <- list("p1" = log(c(5, 1)),
             "p2" = log(c(3, 20)),
             "p3" = log(c(0.5, 0)))

##-------------------------------------------
## Calculando as probabilidades
y <- list(y1 = 0:10, y2 = 0:2, y3 = 0:6)

## COM-Poisson
py.co <- sapply(1:3, function(p)
    dcmp(y[[p]], pars[[p]][1], pars[[p]][2], sumto = 50))
q <- sapply(py.co, length)

da <- data.frame(values = unlist(py.co),
                 y = unlist(y),
                 ind = rep(names(pars), q))

##-------------------------------------------
## Objetos para grafico da lattice
l <- sapply(pars, function(p) exp(p[1]))
n <- sapply(pars, function(p) exp(p[2]))
fl <- substitute(
    expression(lambda == l1~","~nu == n1,
               lambda == l2~","~nu == n2,
               lambda == l3~","~nu == n3),
    list(l1 = l[1], l2 = l[2], l3 = l[3],
         n1 = n[1], n2 = n[2], n3 = n[3]))

##-------------------------------------------
## Grafico
xyplot(values ~ y | ind, data = da,
       type = c("h", "g"),
       xlab = "y", ylab = expression(Pr(Y == y)),
       scales = list(relation = "free", rot = 0),
       layout = c(NA, 1),
       par.strip = list(lines = 2, col = "transparent"),
       sub = "Fonte: Elaborado pelo autor.")
distr <- expression("Poisson", ""%~~%"Bernoulli", "Geométrica")
##-------------------------------------------
## http://stackoverflow.com/questions/33632344/strip-with-two-lines-title-r-lattice-plot
for (i in 1:3) {
    ## navigate to i-th strip
    vp <- paste0("plot_01.strip.", i, ".1.vp")
    grid::downViewport(vp)
    ## add first and second line of text
    grid::grid.text(distr[i], vjust = 0)
    grid::grid.text(eval(fl[[i+1]]), vjust = 1.1)
    ## navigate to top level
    grid::upViewport(0)
}

@

Um inconveniente desse modelo é que os momentos média e variância não
são obtidos em forma fechada. Sendo assim, devem ser calculados a partir
da definição
$$
  \text{E}(Y) = \sum_{y = 0}^{\infty} y \cdot p(y)
    \qquad \textrm{e} \qquad
  \text{V}(Y) = \sum_{y = 0}^{\infty} y^2 \cdot p(y) - E^2(Y)
$$

\citeonline{Shmueli2005}, a partir de uma aproximação para $Z(\lambda,
\nu)$, apresenta uma forma aproximada para os momentos da distribuição
\begin{equation}
  \label{eqn:cmp-mean-aprox}
  \text{E}(Y) \approx \lambda^{1/\nu} - \frac{\nu - 1}{2\nu} \qquad
  \textrm{e} \qquad
  \text{V}(Y) \approx \frac{\lambda^{1/\nu}}{\nu}
\end{equation}
os autores ressaltam que essa aproximação é satisfatória para
$\nu \leq 1$ ou $\lambda > 10^\nu$. Na \autoref{fig:mv-compoisson} é
representada a relação média e variância aproximada pelas expressões em
\ref{eqn:cmp-mean-aprox}. Percebe-se que a relação é praticamente linear
entre média e variância, \citeonline{Sellers2010} descrevem que essa
pode ser relação pode, ainda, ser aproximada por
$\frac{1}{\nu}\text{E}(Y)$. Nessa distribuilção, o parâmetro $\nu$
controla a precisão da distribuição, sendo equidispersa quando
$\nu = 1$, superdispersa quando $\nu < 1$ e subdispersa quando
$\nu > 1$.

<<mv-compoisson, fig.height=4, fig.width=4, fig.cap="Relação média--variância na distribuição COM-Poisson.">>=

##-------------------------------------------
## Parâmetros considerados
nu <- seq(0.3, 5, length.out = 50)
col <- brewer.pal(n = 8, name = "RdBu")
col <- colorRampPalette(colors = col)(length(nu))

##-------------------------------------------
## Etiquetas da legenda
labels <- substitute(
    expression(nu == p1, nu == p2, nu == p3),
    list(p1 = min(nu), p2 = median(nu), p3 = max(nu)))

##-------------------------------------------
## Gráfico

par(mar = c(6.5, 5, 3, 3) + 0.1, las = 1)
## Curva identidade representando a Poisson
curve((nu[1]^0 * (2 * mu + 1) - 1)/(2 * nu[1]^0),
      xlab = "", ylab = "",
      from = 0, to = 10, xname = "mu")
title(ylab = expression(V(X) == frac(nu*(2*E(X) + 1) - 1, 2*nu^2)),
      line = 2.5)
title(xlab = expression(E(X) == lambda^{1/nu} - frac(nu-1, 2*nu)),
      line = 4)
grid()

## Curvas da relação média e variância da COM-Poisson
for (a in seq_along(nu)) {
    curve((nu[a] * (2 * mu + 1) - 1)/(2 * nu[a]^2),
          add = TRUE, xname = "mu", col = col[a], lwd = 2)
}
plotrix::color.legend(
    xl = 11, yb = 2.5, xr = 12, yt = 6.5,
    gradient = "y", align = "rb",
    legend = round(fivenum(nu)[c(1, 3, 5)]),
    rect.col = col)
mtext(text = expression(nu), side = 3, cex = 1.3,
      line = -3.5, at = 11.5)
abline(0, 1)
fonte("Fonte: Elaborado pelo autor.", cex = 0.95)

@

Embora a distribuição COM-Poisson não tenha expressão fechada para a
média, pode-se utilizá-la como distribuição condicional da variável
resposta de contagem em modelos de regressão. Isso é feito incorporando
um preditor linear em $\lambda$ que, mesmo não representando a média,
está associado com a locação da distribuição, ou seja, modela-se a média
indiretamente nessa abordagem. O modelo de regressão é definido com as
variáveis aleatórias condicionalmente independentes
$Y_1, Y_2, \ldots, Y_n$, dado o vetor de covariáveis
$\bm{x}_i^\top = (x_{i1}, x_{i2}, \ldots, x_{ip})$ seguindo um modelo
COM-Poisson de parâmetros $g(\lambda_i) = \bm{x}_i^\top\bm{\beta}$,
$i = 1, 2, \ldots, n$ e $\nu$ comum a todas as observações. Na
\autoref{eqn:reg-compoisson} o modelo é devidamente formulado, conforme
a notação de MLG's
\begin{equation}
  \label{eqn:reg-compoisson}
  \begin{split}
    Y_i \mid & \bm{x}_i \sim \textrm{COM-Poisson}(\lambda_i, \nu) \\
    &\eta(\text{E}(Y_i \mid \bm{x}_i)) = g(\lambda_i) =
    \bm{x}_i^\top\bm{\beta},
  \end{split}
\end{equation}
em que $\eta$ é uma função desconhecida, que representa a média em
termos de $\lambda_i$ e $g$ é uma função de ligação, adotada como
logarítmo nessa trabalho.

O algoritmo para estimação do conjunto de parâmetros
$\bm{\theta} = (\nu, \bm{\beta})$ é obtido pela maximização da
log-verossimilhança que, decorrente da especificação em
\ref{eqn:reg-compoisson}, é dada por
\begin{equation}
  \label{eqn:loglik-compoisson}
  \ell(\nu, \bm{\beta} \mid \bm{y}) = \sum_{i=1}^n y_i
  \log(\lambda_i) - \nu \sum_{i=1}^n \log(y!) - \sum_{i=1}^n
  \log(Z(\lambda_i, \nu))
\end{equation}
e então as estimativas de máxima verossimilhança são
$$
\hat{\bm{\theta}} = (\hat{\nu}, \hat{\bm{\beta}}) =
\underset{(\nu,\,\bm{\beta})}{\textrm{arg max }}
\ell(\nu, \bm{\beta} \mid \bm{y}).
$$

<<constante-z, fig.height=3, fig.width=6.7, fig.cap="Convergência da constante de normalização da COM-Poisson para diferentes conjuntos de parâmetros.">>=

##-------------------------------------------
## Parametros da distribuição
pars <- list("p1" = c(1.362, 0.4), "p2" = c(8, 1), "p3" = c(915, 2.5))

##-------------------------------------------
## Calculando as componentes das constantes
zs <- sapply(pars, function(p) funZ(p[1], p[2]), simplify = FALSE)
da <- plyr::ldply(zs)

##-------------------------------------------
## Objetos para grafico da lattice
l <- sapply(pars, function(p) p[1])
n <- sapply(pars, function(p) p[2])
fl <- substitute(
    expression(lambda == l1~","~nu == n1,
               lambda == l2~","~nu == n2,
               lambda == l3~","~nu == n3),
    list(l1 = l[1], l2 = l[2], l3 = l[3],
         n1 = n[1], n2 = n[2], n3 = n[3]))

##-------------------------------------------
## Gráfico
xyplot(z ~ j | .id, data = da,
       type = c("b", "g"), pch = 19,
       scales = "free",
       ylab = list(
           expression(frac(lambda^j, "(j!)"^nu)),
           rot = 0),
       strip = strip.custom(factor.levels = fl),
       sub = "Fonte: Elaborado pelo autor.")

@

Para avaliação da log-verossimilhança, \autoref{eqn:loglik-compoisson},
a constante de normalização $Z(\lambda, \nu)$, conforme definida em
\ref{eqn:constante-z}, é calculada para cada observação, o que
potencialmente torna o processo de estimação lento. Uma ilustração do
número de incrementos considerados para cálculo da constante $Z(\lambda,
\nu)$ é apresentada na \autoref{fig:constante-z}. Nesta ilustração,
foram utilizados os mesmos parâmetros das distribuições da
\autoref{fig:distr-compoisson}. O número de incrementos necessários para
convergência\footnote{Adotou-se como critério de convergência a
  iteração $j$ tal que $\lambda^j/(j!)^\nu < 0,00001$} de $Z(\lambda,
\nu)$ foram \Sexpr{c(table(da[, ".id"]))} nos primeiro, segundo e
terceiro painéis respectivamente.

Detalhes computacionais do algoritmo de maximização e manipulações
algébricas para eficiência na avaliação da log-verossimilhança no modelo
COM-Poisson são discutidos na \autoref{cap03:metodos}.

\section{Modelos para excesso de zeros}
\label{cap02:zeros}

Problemas com excesso de zeros são comuns em dados de
contagem. Caracteriza-se como excesso de zeros casos em que a quantidade
observada de contagens nulas supera substancialmente aquela esperada
pelo modelo de contagem adotado.

As contagens nulas em dados com excesso de zeros podem ser explicadas de
duas formas distintas. A primeira denomina-se de zeros estruturais,
quando a ocorrência de zero se dá pela ausência de determinada
característica na população e a segunda de zeros amostrais, que ocorrem
segundo um processo gerador de dados de contagem (e.g processo
Poisson). Por exemplo, considerando o número de dias que uma família
consome um determinado produto, têm-se aquelas famílias que não consomem
o produto (zeros estruturais) e as demais famílias que consomem o
produto, porém não o consumiram no intervalo de tempo considerado no
estudo (zeros amostrais). Assim, de forma geral, são dois processos
geradores de dados em uma variável aleatória de contagem com excessivos
zeros.

Em geral, quando dados de contagem apresentam excesso de zeros também
apresentarão superdispersão. Todavia, essa dispersão pode ser
exclusivamente devido ao excesso de zeros, e os modelos alternativos já
apresentados não terão um bom desempenho. Uma ilustração desse fato é
apresentada na \autoref{fig:ilustra-zeros}, em que foram simulados dados
com excesso de zeros. A simulação foi realizada de forma hierárquica,
simulando valores $y_z$ de uma variável aleatória Bernoulli de parâmetro
$\pi$ e, para $y_z=0$ armazenou-se o zero e para $y_z=1$ simulou-se de
uma distribuição Poisson de paramêtro $\lambda$. Ajustando um modelo
COM-Poisson para as duas simulações com diferentes parâmetros $\pi$ e
$\lambda$, observa-se que o modelo não se mostra adequado, indicando que
os excessos de zeros devem ser abordados de forma diferente.

<<ilustra-zeros, fig.cap="Ilustração de dados de contagem com excesso de zeros.", fig.height=3, fig.width=5>>=

##-------------------------------------------
## Simula os dados
set.seed(20124689)
n <- 1000

lambda1 <- 2; pi1 <- 0.9
y1 <- sapply(rbinom(n, 1, pi1), function(x) {
    ifelse(x == 0, 0, rpois(1, lambda1))
})

lambda2 <- 5; pi2 <- 0.85
y2 <- sapply(rbinom(n, 1, pi2), function(x) {
    ifelse(x == 0, 0, rpois(1, lambda2))
})

##-------------------------------------------
## Estimando as probabilidades
library(cmpreg)

sim <- list("s1" = as.integer(y1), "s2" = as.integer(y2))
probs <- sapply(sim, function(y) {
    yu <- 0:max(y)
    ##-------------------------------------------
    m0 <- glm(y ~ 1, family = poisson)
    py_pois <- dpois(yu, exp(m0$coef))
    ##-------------------------------------------
    m1 <- cmp(y ~ 1, data = data.frame(y = y), sumto = 40)
    py_dcmp <- dcmp(yu, loglambda = m1@coef[-1],
                    phi = m1@coef[1], sumto = 40)
    ##-------------------------------------------
    py_real <- c(prop.table(table(y)))
    ##-------------------------------------------
    cbind(yu, py_real, py_pois, py_dcmp)
}, simplify = FALSE)
da <- plyr::ldply(probs, .id="caso")

##-------------------------------------------
## Objetos para grafico da lattice
ylim <- with(da, extendrange(c(0, max(py_real, py_dcmp))))
cols <- trellis.par.get("superpose.line")$col[1:2]
key <- list(
    columns = 2,
    lines = list(lty = 1, col = cols),
    text = list(c("Observado", "COM-Poisson")))
fl <- substitute(
    expression(pi == p1~","~lambda == l1,
               pi == p2~","~lambda == l2),
    list(p1 = pi1, p2 = pi2, l1 = lambda1, l2 = lambda2)
)

##-------------------------------------------
## Gráfico
xyplot(py_real ~ c(yu - 0.15) | caso, data = da,
       type = c("h", "g"),
       xlab = "y",
       ylab = expression(Pr(Y==y)),
       ylim = ylim,
       key = key,
       strip = strip.custom(
           factor.levels = fl
       ),
       sub = "Fonte: Elaborado pelo autor.") +
    as.layer(xyplot(
        py_dcmp ~ c(yu + 0.15) | caso, data = da,
        type = "h", col = cols[2]))

@


\citeonline[capítulo 7]{Hilbe2014} discute sobre a interpretação e
modelagem de dados de contagem com excesso de zeros. Para essa situação
as duas principais abordagens são i) os modelos de mistura
\cite{Lambert1992}, também chamados de inflacionados, em inglês
\textit{Zero Inflated Models} e ii) os modelos condicionais
\cite{Ridout1998}, também chamados de modelos de barreira, em inglês
\textit{Hurdle Models}. Neste trabalho, modela-se o excesso de zeros via
modelos condicionais. A função massa de probabilidade de um modelo de
barreira é
\begin{equation}
  \label{eqn:pmf-hurdle}
  \Pr(Y = y \mid \pi, \Theta_c) =
    \begin{dcases*}
      \pi, & \text{se } y = 0\,;\\
      (1 - \pi) \frac{\Pr(Z = z \mid \theta_c)}{1 - \Pr(Z = 0 \mid
        \theta_c)}, & \text{se } y = 1, 2, \dots,
    \end{dcases*}
\end{equation}
em que $0<\pi<1$, representa a probabilidade de ocorrência de zeros e
$\Pr(Z = z \mid \theta_c)$ a função massa de probabilidade de uma
variável aleatória de contagem $Z$, como a Poisson ou a binomial
begativa.

Da especificação em \ref{eqn:pmf-hurdle}, a média e a variância são
obtidas como
$$
E(Y) = \frac{E(Z)(1-\pi)}{1-\Pr(Z = 0)} \quad \textrm{e} \quad
V(Y) = \frac{1-\pi}{1-\Pr(Z = 0)} \left [ E(Z) \frac{(1-\pi)}{1-\Pr(Z =
    0)} \right ].
$$

Para a inclusão de covariáveis, caracterizando um problema de regressão,
dado que o modelo tem dois processos modela-se ambos como se segue
\begin{equation}
  \label{eqn:reg-hurdle}
  \log \left (\frac{\pi_i}{1-\pi_i} \right ) =
  \bm{\textsf{z}}_i^\top\bm{\gamma} \qquad \textrm{e} \qquad
  \begin{matrix}
    Z_i \sim D(\mu_i, \phi) \\
    g(\mu_i) = \bm{x}_i^\top\bm{\beta},
  \end{matrix}
\end{equation}
sendo $i = 1, 2, \ldots, n$, $\bm{\textsf{z}}_i$ e $\bm{x}_i$ as
covariáveis da i-ésima observação consideradas para as contagens nulas e
não nulas respectivamente, $D(\mu_i, \phi)$ a distribuição de
probabilidades considerada para as contagens não nulas, que pode conter
ou não um parâmetro $\phi$; e $g(\mu_i)$ a função de ligação.  Nos casos
Poisson e binomial begativo, em geral, considera-se
$g(\mu_i) = \log(\mu_i)$. O que está implícito na formulação em
\ref{eqn:reg-hurdle} é que para a componente que explica a geração de
zeros está sendo considerada a distribuição Bernoulli de parâmetro
$\pi_i$ com função de ligação logística. Contudo, pode-se utilizar
distribuições censuradas à direita no ponto $y=1$ para estimação dessa
probabilidade, como explicam \citeonline{Zeileis2007}.

\section{Modelos de efeitos aleatórios}
\label{cap02:aleatorio}

Nas seções anteriores, os modelos flexibilizam algumas suposições do
modelo Poisson, basicamente permitindo casos não equidispersos e
modelando conjuntamente um processo gerador de zeros extra. Contudo, uma
suposição dos modelos de regressão para dados de contagem vistos até
aqui é que as variáveis aleatórias $Y_1, Y_2, \ldots, Y_n$ são
independentes, dado o vetor de covariáveis. Porém, não são raras as
situações em que essa suposição não se mostra
adequada. \citeonline{Ribeiro2012} cita alguns exemplos:
\begin{itemize}
  \setlength\itemsep{0.1em}
  \item as observações podem ser correlacionadas no espaço;
  \item as observações podem ser correlacionadas no tempo;
  \item interações complexas podem ser necessárias para modelar o efeito
    conjunto de algumas covariáveis;
  \item heterogeneidade entre indivíduos ou unidades podem não ser
    suficientemente descrita por covariáveis.
\end{itemize}
Nessas situações, pode-se estender a classe de modelos de regressão com
a inclusão de efeitos aleatórios que incorporam termos baseados em
variáveis não observáveis (latentes), modelando uma fonte de
variabilidade não prescrita pelo modelo. De forma geral, os modelos com
efeitos aleatórios seguem uma especificação hierárquica
\begin{equation}
  \label{eqn:reg-misto}
  \begin{split}
    Y_{ij} \mid b_{i},\,& \bm{x}_{ij} \sim
    \textrm{D}(\mu_{ij}, \phi) \\
    g(&\mu_{ij}) =  \bm{x}_{ij}^\top\bm{\beta} + \bm{z}_i^\top\bm{b}_i\\
    & \bm{b} \sim \textrm{K}(\theta_b),
  \end{split}
\end{equation}
em que $i = 1, 2, \ldots, m$ (grupos com efeitos aleatórios comuns) e
$j = 1, 2, \ldots, n$ (observações); D$(\mu_{ij}, \phi)$, uma
distribuição considerada para as variáveis resposta condicionalmente
independentes; $g(\mu_{ij})$ uma função de ligação conforme definida na
teoria dos MLG's; $\bm{x}_{ij}$ e $\bm{z}_{i}$ os vetores conhecidos que
representam os efeitos das covariáveis de interesse e os termos que
definem os grupos considerados como aleatórios; $\bm{b}_i$ uma
quantidade aleatória provida de uma distribuição K$(\theta_b)$. Nesses
modelos, um termo aleatório é somado ao preditor linear, diferentemente
dos modelos de efeitos fixos, e a partir deste termo é possível induzir
uma estrutura de dependência entre as observações.

Como são dois termos aleatórios no modelo, $Y_{ij}$ condicional ao vetor
de covariáveis e $\bm{b}_i$, a verossimilhança é dada integrando-se os
efeitos aleatórios
\begin{equation}
  \label{eqn:loglik-misto}
  \Ell(\bm{\beta}, \phi, \theta_b \mid \bm{y}, \bm{b}) =
  \prod_{i=1}^m \int_{\R^q}
  \left ( \prod_{j=1}^{n_i} f_D(y_{ij}, \mu, \bm{b}_i)\right ) \cdot
  f_K(\bm{b}_i \mid \theta_b) d\bm{b}_i^\top.
\end{equation}

Na avaliação da verossimilhança é necessário o cálculo de $m$ integrais
de dimensão $q$. Para muitos casos, essa integral não tem forma analítica
sendo necessário métodos numéricos de intergração, que são discutidos
na \autoref{cap03:metodos}. As estimativas de máxima verossimilhança
são
$$
\hat{\bm{\theta}} = (\hat{\bm{\beta}}, \hat{\bm{\theta}_b}) =
\underset{(\bm{\beta},\,\bm{\theta}_b)}{\textrm{arg max }}
\log(\Ell(\bm{\beta}, \phi, \bm{\theta}_b \mid \bm{y}, \bm{b}))
$$

Em modelos de efeitos mistos é comum adotar como distribuição para os
efeitos aleatórios uma Normal $q$-variada com média 0 e matriz de
variâncias e covariâncias $\Sigma$, ou seja, na especificação
\ref{eqn:reg-misto}, K$(\theta_b) = \mathcal{N}_q(\bm{0}, \Sigma)$.

Como mencionado anteriormente, modelos de efeitos aleatórios são
candidatos à modelagem de dados superdispersos. Quando não há uma
estrutura de delineamento experimental ou observacional, pode-se incluir
efeitos aleatórios em nível de observação (e então $m=n$, ou seja, os
vetores $\bm{y}$ e $\bm{b}$ tem mesma dimensão) capturando uma
variabilidade extra. São casos assim os modelos binomial negativo e
Poisson inverso-Gaussiano, ambos são definidos a partir de efeitos
aleatórios multiplicativos em nível de observação e a integral, definida
na \autoref{eqn:loglik-misto}, tem solução analítica, consequentemente,
a marginal em $Y$ tem forma fechada.
